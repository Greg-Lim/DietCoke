

# Img2LLM
We propose Img2LLM, a plug-and-play module that converts an image into synthetic question-answer pairs based solely on the current image of the question. Img2LLM bridges the modality disconnect between language and vision as well as the task disconnect between language modeling and visual question answering

JiaxianGuo07@gmail.com

Dear Jiaxian,

I am a undergraduate student at the Nanyang Technological University, Singapore. I am working on a project that reference a paper that referenced Img2LLM. I would like to ask how or where the captions and QA pairs that are used Img2LLM are from/generated. Here is the link to the data of intrest, https://github.com/CR-Gjx/Img2Prompt/blob/5b838d1650e30495139325c0bb98294c179ecedc/README.md?plain=1#L103.

https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2llm-vqa/img2llm_vqa.ipynb


git submodule add https://github.com/CR-Gjx/Img2Prompt.git VL_captioning



Hi Hongrui, I want to am facing an issue when integrating the LAVIS to DietCoke Pipeline.